{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote_notebook_id": "b81be983-d1f9-4021-8495-d5645d59ff2d",
    "deepnote_execution_queue": [],
    "colab": {
      "name": "RhinoChess.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R82sk6CTEeee"
      },
      "source": [
        "# Introduction\r\n",
        "---\r\n",
        "**Rhino** is a deep learning chess engine largely inspired by the [DeepChess](http://www.cs.tau.ac.il/~wolf/papers/deepchess.pdf) paper released in 2016. A CNN has been add among other modifications to optimize learning. If you wish to read more about the higher level details you can read [this article](http://) discussing these topics. Below is everything it takes to play Rhino or build it from scratch.\r\n",
        "\r\n",
        "Enjoy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFqwGuUQPHZb"
      },
      "source": [
        "# Imports and Google Drive Mount\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00000-28f97962-050e-4d9b-ab66-e316f4d523ca",
        "output_cleared": false,
        "source_hash": "ebf29d11",
        "execution_millis": 3236,
        "execution_start": 1609037994547,
        "deepnote_cell_type": "code",
        "id": "pXA30vTbLOX_"
      },
      "source": [
        "import chess\n",
        "import chess.svg\n",
        "import chess.pgn \n",
        "import numpy as np \n",
        "import random\n",
        "import h5py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import SVG, display\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT7yrOALLpBS",
        "outputId": "a9c89726-c03e-4c72-d226-2105a24cc847"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path='/content/drive/MyDrive/Colab_Static/Rhino/'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00001-47090998-2ab7-4238-af40-0921789591e8",
        "output_cleared": false,
        "deepnote_cell_type": "markdown",
        "id": "-DLkKOdALOYN"
      },
      "source": [
        "# Getting Data\n",
        "---\n",
        "Load/format training data from the \n",
        "[CCRL dataset](http://ccrl.chessdom.com/ccrl/4040/games.html). \n",
        "Considering only non-draw games, we select 10 random positions within the game and add them to our training set. Positions are selected such that it wasn't within the first 5 moves and the previous move was not a capture. \n",
        "\n",
        "\n",
        "An instance is a 773 long array and its associated label is the outcome of the game\n",
        "(1 = White wins, 0 = Black wins).\n",
        "\n",
        "Our end dataset contains two million positions, half from games that white won and the other half from games black won. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00001-c92f0c21-3937-4e47-84e4-00b9a518e08e",
        "output_cleared": false,
        "source_hash": "ca138790",
        "execution_millis": 0,
        "execution_start": 1608830461500,
        "deepnote_cell_type": "code",
        "id": "IJfGQCqDLOYO"
      },
      "source": [
        "def get_positions(game,num_pos=10):\n",
        "   legal_positions = []\n",
        "   board = game.board()\n",
        "   for i,move in enumerate(game.mainline_moves()):\n",
        "      if board.is_capture(move) or i<5:\n",
        "         board.push(move)\n",
        "      else: #legal\n",
        "         board.push(move) \n",
        "         legal_positions.append(board)\n",
        "   return random.sample(legal_positions,num_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00004-c90a52f8-6570-4514-b696-f38e1de7bd0b",
        "output_cleared": false,
        "source_hash": "e09d270",
        "execution_millis": 1,
        "execution_start": 1608830466196,
        "deepnote_cell_type": "code",
        "id": "qpF7-aQZLOYP"
      },
      "source": [
        "def to_bitboard(board):\n",
        "   bb = np.zeros((2,6,64),dtype=np.uint8) # players x pieces x board-size    \n",
        "   for colour in range(2):\n",
        "      for piece in range(6):\n",
        "         for square in range(64):\n",
        "            cur_piece = board.piece_at(square)\n",
        "            if cur_piece is not None:\n",
        "               if cur_piece.piece_type == piece+1 and cur_piece.color == bool(colour):\n",
        "                  bb[colour][piece][square] = 1\n",
        "   \n",
        "   info = np.zeros(5,dtype=np.uint8)\n",
        "   info[0] = board.has_kingside_castling_rights(chess.WHITE)\n",
        "   info[1] = board.has_kingside_castling_rights(chess.BLACK)\n",
        "   info[2] = board.has_queenside_castling_rights(chess.WHITE)\n",
        "   info[3] = board.has_queenside_castling_rights(chess.BLACK)   \n",
        "   info[4] = board.turn\n",
        "\n",
        "   bb = bb.flatten()\n",
        "   bb = np.concatenate((bb,info))\n",
        "   return bb"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00003-d8207369-f1f7-4811-af65-381caa9379a3",
        "output_cleared": false,
        "source_hash": "361de2ef",
        "execution_millis": 1,
        "execution_start": 1608833204316,
        "deepnote_cell_type": "code",
        "id": "sAMPFfnbLOYQ"
      },
      "source": [
        "def preprocess(num_samples,fn='CCRL-4040.[1199203].pgn'):\n",
        "   X_win,Y_win,X_lose,Y_lose = [],[],[],[]\n",
        "   n,m = 0,0\n",
        "   all_games = open(fn)\n",
        "   while n<num_samples or m<num_samples:\n",
        "      game = chess.pgn.read_game(all_games)\n",
        "      if game == None: # EOF\n",
        "         print(\"EOF:\", n, \"games obtained\")\n",
        "      if game.headers['Result'] != '1-1': # not a draw         \n",
        "         boards = get_positions(game)\n",
        "         for board in boards:\n",
        "            if game.headers['Result'] == '1-0' and n<num_samples:\n",
        "               n+=1\n",
        "               X_win.append(to_bitboard(board)) # white win\n",
        "               Y_win.append(1)\n",
        "            elif m<num_samples: \n",
        "               m+=1\n",
        "               X_lose.append(to_bitboard(board)) # white lose \n",
        "               Y_lose.append(0)\n",
        "   return X_win,Y_win,X_lose,Y_lose"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00008-c4b12cde-04b6-44e9-ba12-4afe689f8cdc",
        "output_cleared": false,
        "source_hash": "560bf541",
        "execution_millis": 4943069,
        "execution_start": 1608833296456,
        "deepnote_cell_type": "code",
        "id": "MAAICjZKLOYR",
        "outputId": "474b5522-ae10-4e6c-9866-8037b3b98472"
      },
      "source": [
        "# Collect Data\n",
        "X_win,Y_win,X_lose,Y_lose = preprocess(1000e3)\n",
        "assert len(X_win) == len(Y_win) and len(X_win) == len(Y_win)\n",
        "print(\"Number of winning games (for White)\",len(X_win))\n",
        "print(\"Number of winning games (for Black)\",len(X_lose))\n",
        "# Save Data\n",
        "hf = h5py.File('data_1M.h5','w')\n",
        "hf.create_dataset('X_win',data=X_win)\n",
        "hf.create_dataset('Y_win',data=Y_win)\n",
        "hf.create_dataset('X_lose',data=X_lose)\n",
        "hf.create_dataset('Y_lose',data=Y_lose)\n",
        "hf.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of winning games (for White) 1000000\n",
            "Number of winning games (for Black) 1000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00006-df86c65d-aa9b-4f99-8415-5fdbec844b4d",
        "deepnote_cell_type": "markdown",
        "id": "7gb9RWaVLOYU"
      },
      "source": [
        "# Building Tensor Datasets\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00007-4541ce14-c075-4639-b535-96463d64e8d7",
        "output_cleared": false,
        "source_hash": "62890b14",
        "execution_millis": 2542,
        "execution_start": 1609038012892,
        "deepnote_cell_type": "code",
        "id": "RyASVyEkLOYU"
      },
      "source": [
        "# Load Data and Compute Train/Test Splits\n",
        "hf = h5py.File(path+'data_1M.h5','r')\n",
        "X_win_train,X_win_val,Y_win_train,Y_win_val = train_test_split(np.array(hf.get('X_win')), \n",
        "                                                   np.array(hf.get('Y_win')), \n",
        "                                                   test_size=0.1, random_state=2)\n",
        "X_lose_train,X_lose_val,Y_lose_train,Y_lose_val = train_test_split(np.array(hf.get('X_lose')), \n",
        "                                                   np.array(hf.get('Y_lose')), \n",
        "                                                   test_size=0.1, random_state=2)\n",
        "\n",
        "hf.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx5lgjxYSmuG"
      },
      "source": [
        "# Convert Data To Tensors \r\n",
        "\r\n",
        "# Train\r\n",
        "X_win_train = torch.tensor(X_win_train).float()\r\n",
        "Y_win_train = torch.tensor(Y_win_train)\r\n",
        "winData_train = TensorDataset(X_win_train,Y_win_train)\r\n",
        "\r\n",
        "X_lose_train = torch.tensor(X_lose_train).float()\r\n",
        "Y_lose_train = torch.tensor(Y_lose_train)\r\n",
        "loseData_train = TensorDataset(X_lose_train,Y_lose_train)\r\n",
        "\r\n",
        "# Val\r\n",
        "X_win_val = torch.tensor(X_win_val).float()\r\n",
        "Y_win_val = torch.tensor(Y_win_val)\r\n",
        "winData_val = TensorDataset(X_win_val,Y_win_val)\r\n",
        "\r\n",
        "X_lose_val = torch.tensor(X_lose_val).float()\r\n",
        "Y_lose_val = torch.tensor(Y_lose_val)\r\n",
        "loseData_val = TensorDataset(X_lose_val,Y_lose_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk2xlECFVM9G"
      },
      "source": [
        "# Model\r\n",
        "---\r\n",
        "The architecture for Rhino is largely inspired by [DeepChess](http://www.cs.tau.ac.il/~wolf/papers/deepchess.pdf) with a CNN replacing the autoencoders in the *Pos2Vec* module. The goal of this network is to determine which of two given positions is more preferable from white's perspective. There are two components:\r\n",
        "\r\n",
        "1.   **Position Convertor**: This compares to the *Pos2Vec* module in the paper. The objective is to convert a board ($2 \\times 6 \\times 64$ array) into a 1D vector of length 100. In essence we are breaking down a given position into its key components. \r\n",
        "\r\n",
        "2.   **Position Comparator**: Given two converted boards we include some extra information carried in the last five bits (castling rights and turn). Using three fully connected layers the network deduces which of the two positions are more beneficial for white. Note that this is similar to the *DeepChess* module in the paper.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voEx-MGgVMJD"
      },
      "source": [
        "class RhinoNN(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(RhinoNN, self).__init__()\r\n",
        "        self.p2v = nn.Sequential(\r\n",
        "            nn.Conv2d(2,32,3),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "\r\n",
        "            nn.Conv2d(32,64,3),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.MaxPool2d(2,2)\r\n",
        "        )\r\n",
        "        self.p2v_last = nn.Linear(1920,100)\r\n",
        "        self.comp_pos = nn.Sequential(\r\n",
        "            nn.Linear(210,400),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Linear(400,200),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Linear(200,100),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Linear(100,2)\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self,x1,x2):\r\n",
        "        x1_b, x1_i = x1[:768], x1[768:]\r\n",
        "        x2_b, x2_i = x2[:768], x2[768:]\r\n",
        "        \r\n",
        "        x1_b = self.p2v(x1_b.view(1,2,6,64))\r\n",
        "        x1_b = self.p2v_last(x1_b.view(1,1920))\r\n",
        "\r\n",
        "        x2_b = self.p2v(x2_b.view(1,2,6,64))\r\n",
        "        x2_b = self.p2v_last(x2_b.view(1,1920))\r\n",
        "\r\n",
        "        x1 = torch.cat((x1_b,x1_i.unsqueeze(0)),1)\r\n",
        "        x2 = torch.cat((x2_b,x2_i.unsqueeze(0)),1)\r\n",
        "\r\n",
        "        x = torch.cat((x1,x2),1)\r\n",
        "        x = self.comp_pos(x)\r\n",
        "        return F.softmax(x,dim=1)\r\n",
        "rhinoNN = RhinoNN()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEpQ5FC7FXT_"
      },
      "source": [
        "# Train & Test\r\n",
        "---\r\n",
        "Upon every epoch we select $25,000$ winning and losing positions to form a set of `(W,L)` pairs. The goal is training Rhino to recognize winning positions. We randomly reverse a subset of the board position pairs `(L,W)` to avoid a bias towards one class. So there are $2\\times 10^{12}$ such pairs which avoids overfitting for relatively low `MAX_EPOCHS` values. \r\n",
        "\r\n",
        "At the end of every epoch we multiply the learning rate by $0.98$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00018-5ee73377-da71-41c9-a2e8-f68c016ed7ca",
        "output_cleared": false,
        "source_hash": "482f3316",
        "execution_millis": 2901,
        "execution_start": 1609030196467,
        "deepnote_cell_type": "code",
        "id": "nsggCdNoLOYe"
      },
      "source": [
        "rhinoNN = RhinoNN()\n",
        "#rhinoNN.load_state_dict(torch.load(path+'rhinoNN_100epoch.pth'))\n",
        "lr_h = 1e-5\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(rhinoNN.parameters(),lr=lr_h)\n",
        "\n",
        "MAX_EPOCHS = 50\n",
        "accuracy = []\n",
        "DATASET_RANGE_W = range(len(winData_train))\n",
        "DATASET_RANGE_L = range(len(loseData_train))\n",
        "k = 25000\n",
        "for epoch in range(MAX_EPOCHS):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    rhinoNN.train()\n",
        "\n",
        "    # Selects k Random Samples\n",
        "    idx_w = random.sample(DATASET_RANGE_W,k)\n",
        "    idx_l = random.sample(DATASET_RANGE_L,k)\n",
        "    \n",
        "    for i in range(k):\n",
        "        input_w,_ = winData_train[idx_w[i]]\n",
        "        input_l,_ = loseData_train[idx_l[i]]\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward + backward + optimize\n",
        "        if random.randint(1,2) == 1:\n",
        "            # Reverse wins and losses\n",
        "            output = rhinoNN(input_l,input_w)\n",
        "            loss = criterion(output, torch.tensor([[0,1]]).float())\n",
        "        else:    \n",
        "            output = rhinoNN(input_w,input_l)\n",
        "            loss = criterion(output, torch.tensor([[1,0]]).float())\n",
        "            \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "            \n",
        "    # Print Statistics    \n",
        "    print('Epoch: %d of %d, loss: %.4f' %\n",
        "         (epoch + 1, MAX_EPOCHS, running_loss / k))\n",
        "    \n",
        "    # Decrease Learning Rate\n",
        "    lr_h = lr_h * 0.98\n",
        "    for g in optimizer.param_groups:\n",
        "        g['lr'] = lr_h\n",
        "\n",
        "\n",
        "print('Finished Training')\n",
        "torch.save(rhinoNN.state_dict(), path+'rhinoNN_'+str(MAX_EPOCHS)+'epoch.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zn2bQDOf8cs"
      },
      "source": [
        "def test_on_validation(model,winData=winData_val,loseData=loseData_val):\r\n",
        "    model.eval()\r\n",
        "    correct = 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for (data1,data2) in zip(winData,loseData):   \r\n",
        "            input_w,_ = data1\r\n",
        "            input_l,_ = data2\r\n",
        "\r\n",
        "            if random.randint(1,2) == 1:\r\n",
        "                # Reverse wins and losses\r\n",
        "                output = model(input_l,input_w)\r\n",
        "                if torch.argmax(output) == torch.tensor(1):\r\n",
        "                    correct += 1\r\n",
        "            else:    \r\n",
        "                output = model(input_w,input_l)\r\n",
        "                if torch.argmax(output) == torch.tensor(0):\r\n",
        "                    correct += 1\r\n",
        "            \r\n",
        "        acc = 100 * correct / len(winData_val)\r\n",
        "        print('Accuracy on Validation Set:', acc,'%')\r\n",
        "        return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnRVwf1ykv1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f748af9-8293-462b-abe5-51b13f3a29ec"
      },
      "source": [
        "test_on_validation(rhinoNN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on Validation Set: 97.173 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97.173"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M_NCDtXEKXw"
      },
      "source": [
        "# Comparison Based Alpha-Beta Search\r\n",
        "---\r\n",
        "A modification of the Alpha-Beta Search Algorithm proposed in the DeepChess paper. \r\n",
        "> In order to incorporate DeepChess, we use a novel version of an alpha-beta algorithm that does not require any position scores for performing the search. Instead of $\\alpha$ and $\\beta$ values, we store positions $\\alpha_{pos}$ and $\\beta_{pos}$. For each new position, we compare it with the existing $\\alpha_{pos}$ and $\\beta_{pos}$ positions using DeepChess, and if the comparison\r\n",
        "shows that the new position is better than $\\alpha_{pos}$, it would become the new $\\alpha_{pos}$, and if the new position is better than $\\beta_{pos}$, the current node is pruned. Note that since DeepChess always compares the positions from White’s perspective, when using it from Black’s perspective, the predictions should be reversed.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xz0dssoEQC7"
      },
      "source": [
        "def alphabeta(node, depth, alpha, beta, isMaximizingPlayer):\r\n",
        "    if depth == 0:\r\n",
        "        return node\r\n",
        "    if isMaximizingPlayer:\r\n",
        "        best = -1\r\n",
        "        for move in node.legal_moves:\r\n",
        "            new_node = node.copy()\r\n",
        "            new_node.push(move)\r\n",
        "            val = alphabeta(new_node,depth-1,alpha,beta,False)\r\n",
        "\r\n",
        "            # Conversion of max(best,val)\r\n",
        "            if best == -1:\r\n",
        "                best = val\r\n",
        "            else:\r\n",
        "                best = rhinoPredict(best,val)[0]\r\n",
        "            \r\n",
        "            # Conversion of max(alpha,best)\r\n",
        "            if alpha == -1:\r\n",
        "                alpha = best\r\n",
        "            else:\r\n",
        "                alpha = rhinoPredict(alpha,best)[0]\r\n",
        "\r\n",
        "            if beta != 1:\r\n",
        "                if rhinoPredict(alpha,beta)[0] == alpha:\r\n",
        "                    break\r\n",
        "        return best\r\n",
        "    else: \r\n",
        "        best = 1\r\n",
        "        for move in node.legal_moves:\r\n",
        "            new_node = node.copy()\r\n",
        "            new_node.push(move)\r\n",
        "            val = alphabeta(new_node,depth-1,alpha,beta,True)\r\n",
        "            \r\n",
        "            # Conversion of min(best,val)\r\n",
        "            if best == 1:\r\n",
        "                best = val \r\n",
        "            else:\r\n",
        "                best = rhinoPredict(best,val)[1]\r\n",
        "\r\n",
        "            # Conversion of min(beta,best)\r\n",
        "            if beta == 1: \r\n",
        "                beta = best\r\n",
        "            else:\r\n",
        "                beta = rhinoPredict(beta,best)[1]\r\n",
        "\r\n",
        "            if alpha != -1:\r\n",
        "                if rhinoPredict(alpha,beta)[0] == alpha:\r\n",
        "                      break\r\n",
        "\r\n",
        "        return best\r\n",
        "\r\n",
        "def rhinoPredict(b1,b2):\r\n",
        "    x1 = torch.tensor(to_bitboard(b1)).float()\r\n",
        "    x2 = torch.tensor(to_bitboard(b2)).float()\r\n",
        "    output = rhino(x1,x2)\r\n",
        "    if torch.argmax(output) == torch.tensor(0):\r\n",
        "        return (b1,b2)\r\n",
        "    else:\r\n",
        "        return (b2,b1)\r\n",
        "  "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwgZ2qcUNlsv"
      },
      "source": [
        "# Game Interface\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rnXzaWvzc-k"
      },
      "source": [
        "def showBoard(b,flip=False):\r\n",
        "    display(SVG(chess.svg.board(b,size=300,flipped=flip)))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ3ADtxeznNZ"
      },
      "source": [
        "def findMoveInBook(b):\r\n",
        "    opening_book = open(path+'8moves_v3.pgn')\r\n",
        "    game = chess.pgn.read_game(opening_book)\r\n",
        "    b_temp = game.board()\r\n",
        "    while True:\r\n",
        "        for move in game.main_line():\r\n",
        "            if len(b_temp.move_stack) == len(b.move_stack):\r\n",
        "                if b_temp == b:\r\n",
        "                    return move\r\n",
        "                break\r\n",
        "            b_temp.push(move)\r\n",
        "        game = chess.pgn.read_game(opening_book)\r\n",
        "        if game is None: #EOF\r\n",
        "            break\r\n",
        "        b_temp = game.board()\r\n",
        "    return None"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV-d0O_ATCco"
      },
      "source": [
        "def computerMove(board,depth,isWhite):\r\n",
        "    # Check Opening Book\r\n",
        "    if len(board.move_stack) < 8:\r\n",
        "        bookMove = findMoveInBook(board)\r\n",
        "        if bookMove is not None:\r\n",
        "            print('Book Move Found: ', bookMove)\r\n",
        "            board.push(bookMove)\r\n",
        "            full_game.add_main_variation(bookMove)\r\n",
        "            return board  \r\n",
        "    print('No Book Moves Found, Using Engine...')\r\n",
        "    alpha = -1 \r\n",
        "    beta = 1\r\n",
        "    best = -1\r\n",
        "    if isWhite:\r\n",
        "        for move in board.legal_moves:\r\n",
        "            new_board = board.copy()\r\n",
        "            new_board.push(move)\r\n",
        "            if best == -1:\r\n",
        "                best = alphabeta(new_board,depth-1,alpha,beta,True) \r\n",
        "                bestMove = move\r\n",
        "                if beta == 1:\r\n",
        "                    beta = best\r\n",
        "            else:\r\n",
        "                new_best = rhinoPredict(best,alphabeta(new_board,depth-1,alpha,beta,True))[1]\r\n",
        "                if new_best != best:\r\n",
        "                    bestMove = move\r\n",
        "                    best = new_best\r\n",
        "                beta = rhinoPredict(beta,best)[1]\r\n",
        "\r\n",
        "    else:\r\n",
        "        for move in board.legal_moves:\r\n",
        "            new_board = board.copy()\r\n",
        "            new_board.push(move)\r\n",
        "            if best == -1:\r\n",
        "                best = alphabeta(new_board,depth-1,alpha,beta,False) \r\n",
        "                bestMove = move\r\n",
        "                if alpha == -1:\r\n",
        "                    alpha = best\r\n",
        "            else:\r\n",
        "                new_best = rhinoPredict(best,alphabeta(new_board,depth-1,alpha,beta,False))[0]\r\n",
        "                if new_best != best:\r\n",
        "                    bestMove = move\r\n",
        "                    best = new_best\r\n",
        "                alpha = rhinoPredict(alpha,best)[0] \r\n",
        "     \r\n",
        "    print(bestMove)\r\n",
        "    board.push(bestMove)\r\n",
        "    full_game.add_main_variation(bestMove)\r\n",
        "    return board\r\n",
        "\r\n",
        "def playerMove(board):\r\n",
        "      while True:\r\n",
        "          try:\r\n",
        "              move = input('Enter a move:\\n')\r\n",
        "              board.push_san(move)\r\n",
        "              break\r\n",
        "          except ValueError:\r\n",
        "              print('Illegal move, try again')\r\n",
        "      return board      "
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaBsinerz-cz"
      },
      "source": [
        "def playGame(rhino_dict='rhinoNN_125epoch.pth'):\r\n",
        "    global rhino \r\n",
        "    global full_game\r\n",
        "  \r\n",
        "    rhino = RhinoNN()\r\n",
        "    rhino.load_state_dict(torch.load(path+rhino_dict))\r\n",
        "    full_game = chess.pgn.Game()\r\n",
        "\r\n",
        "    board = chess.Board()\r\n",
        "\r\n",
        "    max_depth = int(input(\"Set Rhino's Maximum Search Depth\\n\"))\r\n",
        "    game_type = int(input(\"Enter '1' for You VS. Rhino \\nEnter '2' for Rhino VS. Rhino\\n\"))\r\n",
        "\r\n",
        "    # ------------ USER VS. RHINO ------------\r\n",
        "    if game_type == 1:\r\n",
        "        game.headers[\"Event\"] = \"Human vs. Rhino\"\r\n",
        "        isPlayerTurn = bool(random.randint(0,1))\r\n",
        "        computerColour = not isPlayerTurn\r\n",
        "        while board.is_game_over() == False:\r\n",
        "            showBoard(board,flip=computerColour)\r\n",
        "            if isPlayerTurn:\r\n",
        "                board = playerMove(board)\r\n",
        "            else:\r\n",
        "                board = computerMove(board,max_depth,computerColour) \r\n",
        "\r\n",
        "            isPlayerTurn = not isPlayerTurn  \r\n",
        "        showBoard(board,flip=computerColour) # show final position\r\n",
        "\r\n",
        "    # ------------ RHINO VS. RHINO ------------        \r\n",
        "    elif game_type == 2:\r\n",
        "        game.headers[\"Event\"] = \"Rhino vs. Rhino\"\r\n",
        "        whiteToMove = True\r\n",
        "        while board.is_game_over() == False:\r\n",
        "            showBoard(board)\r\n",
        "            if whiteToMove:\r\n",
        "                board = computerMove(board,max_depth,True)\r\n",
        "            else:\r\n",
        "                board = computerMove(board,max_depth,False)\r\n",
        "            \r\n",
        "            whiteToMove = not whiteToMove\r\n",
        "        showBoard(board) # show final position\r\n",
        "\r\n",
        "    # Print Results\r\n",
        "    if board.is_checkmate():\r\n",
        "        print(\"Checkmate\")\r\n",
        "    elif board.is_stalemate():\r\n",
        "        print(\"Draw: Stalemate\")\r\n",
        "    elif board.is_insufficient_material():\r\n",
        "        print(\"Draw: Insufficient Material\")\r\n",
        "\r\n",
        "    print('PGN:\\n',full_game)\r\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfggaWZTN8he"
      },
      "source": [
        "# Play Rhino\r\n",
        "---\r\n",
        "I have implemented two options to use Rhino. By entering `1`  you can play against Rhino. Enter `2` to watch Rhino play itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWQ6b1CNXgFt"
      },
      "source": [
        "playGame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAJ5fzCgsASl"
      },
      "source": [
        "# Final Thoughts\r\n",
        "---\r\n",
        "General Notes\r\n",
        "* Rhino becomes incredibly slow for depths over 3. \r\n",
        "* Seems to have a low understanding of piece value "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yja8RoJzEKyn"
      },
      "source": [
        "# Resouces\r\n",
        "---\r\n",
        "* Model Architecture: [DeepChess]()\r\n",
        "* Data: [CCRL]()\r\n",
        "* Game Interface: [Oripress/DeepChess](https://github.com/oripress/DeepChess)\r\n",
        "* Alpha-Beta Search: [Chess Programming Wiki](https://www.chessprogramming.org/Alpha-Beta)\r\n",
        "* Opening Book: [Stockfish 8moves_v3](https://github.com/official-stockfish/books)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qALAX5x7H25A"
      },
      "source": [
        "# To Do\r\n",
        "---\r\n",
        "* Larger Opening Book\r\n",
        "* Endgame Table\r\n",
        "* Train on longer (200+ epochs)\r\n",
        "* Position Hashing (speed performance)\r\n",
        "* Change bitboard representation?"
      ]
    }
  ]
}